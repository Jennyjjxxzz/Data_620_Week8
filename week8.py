# -*- coding: utf-8 -*-
"""Week8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JDP95v_PefaH5eTevNYMrGdAFyhTMbgD

## Assignment ‚Äì High Frequency Words

Author: Jiaxin Zheng

1. Choose a corpus of interest.
2. How many total unique words are in the corpus? (Please feel free to define unique words in any interesting, defensible way).
3. Taking the most common words, how many unique words represent half of the total words in the corpus?
4. Identify the 200 highest frequency words in this corpus.
5. Create a graph that shows the relative frequency of these 200 words.
6. Does the observed relative frequency of these words follow Zipf‚Äôs law? Explain.
7. In what ways do you think the frequency of the words in this corpus differ from ‚Äúall words in all corpora.‚Äù

## About data:
- This data is a full text datset of book "Pride and Prejudice" by Jane Austen. sourced from Project Gutenberg. https://www.gutenberg.org/ebooks/1342. It contains over 120,000 words.
"""

import requests
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from nltk.tokenize import RegexpTokenizer
from nltk import FreqDist
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
import networkx as nx
from collections import defaultdict
from itertools import combinations

#read the text file from url github
url = 'https://raw.githubusercontent.com/Jennyjjxxzz/Data_620_Week8/main/Pride%20and%20Prejudice.txt'
response = requests.get(url)

text = response.text.lower()  # convert to lowercase

print(text[:1000])

# Clean the data
start_marker = "*** start of the project gutenberg ebook pride and prejudice ***"
end_marker = "*** end of the project gutenberg ebook pride and prejudice ***"
start_idx = text.find(start_marker)
end_idx = text.find(end_marker)

if start_idx != -1 and end_idx != -1:
    text = text[start_idx + len(start_marker):end_idx].strip()

# Tokenize words
tokenizer = RegexpTokenizer(r'\b\w+\b')
tokens = tokenizer.tokenize(text)

"""## Clean the data, remove stopwords"""

# Remove stopwords
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word not in stop_words and word not in custom_exclude]

"""## Number of unique words
- There are total 6895 unique words
"""

freq_dist = FreqDist(filtered_tokens)
total_words = sum(freq_dist.values())

unique_word_count = len(freq_dist)
unique_word_count

"""## Number of unique words that make up half the corpus"""

cumulative = 0
num_words_half = 0
for word, count in freq_dist.most_common():
    cumulative += count
    num_words_half += 1
    if cumulative >= total_words / 2:
        break
print("Unique Words to Reach 50% of Total:", num_words_half)

"""## Top 200 frequent word
- Shows the top 20 most frequently used meaningful words. Character names like mr, elizabeth, and darcy dominate the narrative, reflecting the focus of the story.
"""

print("Top 200 Frequent Words:\n", top_200_df)

freq_dist = FreqDist(filtered_tokens)
top_200_df = pd.DataFrame(freq_dist.most_common(200), columns=["word", "count"])
top_words_set = set(top_200_df["word"])
# Co-occurrence calculation
window_size = 5
co_occurrence = defaultdict(int)
for i in range(len(filtered_tokens) - window_size + 1):
    window = filtered_tokens[i:i + window_size]
    filtered_window = [word for word in window if word in top_words_set]
    for word1, word2 in combinations(set(filtered_window), 2):
        if word1 != word2:
            pair = tuple(sorted((word1, word2)))
            co_occurrence[pair] += 1

# Build graph
G = nx.Graph()
for (word1, word2), weight in co_occurrence.items():
    if weight > 2:  # filter weak edges
        G.add_edge(word1, word2, weight=weight)

# Draw network
plt.figure(figsize=(16, 12))
pos = nx.spring_layout(G, k=0.3)
nx.draw_networkx_nodes(G, pos, node_size=[freq_dist[w]*0.2 for w in G.nodes()],
                       node_color=[freq_dist[w] for w in G.nodes()], cmap=plt.cm.plasma)
nx.draw_networkx_edges(G, pos, alpha=0.3)
nx.draw_networkx_labels(G, pos, font_size=9)
plt.title("Word Co-occurrence Network (Top 200 Words)")
plt.axis('off')
plt.show()

# Top 10 words in the network by number of connections (degree centrality)
degree_dict = dict(G.degree())
top_10_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:20]
print("\nüîó Top 10 Words by Connections in Co-occurrence Network:")
for word, degree in top_10_nodes:
    print(f"{word}: {degree} connections")

# Barplot of Top 20 Words
top_200_df = pd.DataFrame(freq_dist.most_common(200), columns=["word", "count"])
top_200_df["relative_frequency"] = top_200_df["count"] / total_words

top_20_df = top_200_df.head(20)
plt.figure(figsize=(12, 6))
plt.bar(top_20_df["word"], top_20_df["relative_frequency"])
plt.title("Top 20 Most Frequent Words in 'Pride and Prejudice' (Cleaned)")
plt.xlabel("Word")
plt.ylabel("Relative Frequency")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## Zipf's Law Log-Log Plot
- The first few words (e.g., ‚Äúmr‚Äù, ‚Äúelizabeth‚Äù, ‚Äúsaid‚Äù) are very frequent as you go further down the rank, word frequency drops sharply
"""

# Zipf‚Äôs Law Log-Log Plot
plt.figure(figsize=(8, 6))
plt.loglog(range(1, 201), top_200_df["relative_frequency"].values[:200], marker='o')
plt.title("Zipf‚Äôs Law Log-Log Plot (200 Words)")
plt.xlabel("Log(Rank)")
plt.ylabel("Log(Frequency)")
plt.grid(True)
plt.show()

"""## Observations (After Removing Stopwords):
- More meaningful nouns and verbs rise to the top (e.g., character names, key actions).
- Better reflection of the actual content and themes of the novel.
- Zipf‚Äôs law still generally holds with cleaned vocabulary.


"""